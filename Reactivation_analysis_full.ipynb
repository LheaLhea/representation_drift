{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plot\n",
    "import importlib\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import glob\n",
    "import h5py\n",
    "import math\n",
    "import scipy\n",
    "import random\n",
    "import classify\n",
    "import warnings\n",
    "import preprocess\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.lines as mlines\n",
    "from os import path\n",
    "from scipy import stats\n",
    "from scipy.io import loadmat\n",
    "from matplotlib import gridspec\n",
    "from numpy.linalg import norm\n",
    "from scipy.spatial import distance\n",
    "from sklearn import decomposition\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "matplotlib.rcParams['pdf.fonttype'] = 42\n",
    "matplotlib.rcParams['ps.fonttype'] = 42\n",
    "warnings.filterwarnings('ignore')\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing and getting mean SER and mean Reactivation data for both stimuli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload when updating code\n",
    "importlib.reload(preprocess)\n",
    "# mouse and date\n",
    "\n",
    "mice = ['NN11'] \n",
    "dates_per_mouse = {'NN11': ['210626']}\n",
    "\n",
    "for mouse in mice:\n",
    "    dates = dates_per_mouse[mouse]  # Get the list of dates for the current mouse\n",
    "    days = len(dates)  # Correct number of days for this mouse\n",
    "    \n",
    "    for day_idx, date in enumerate(dates):\n",
    "        print(mouse, date)\n",
    "        \n",
    "        # Create folders to save files\n",
    "        paths = preprocess.create_folders(mouse, date)\n",
    "        print('folders created')\n",
    "        \n",
    "        # Import data for mouse and date as dict\n",
    "        session_data = preprocess.load_data(paths)\n",
    "        print('data loaded')\n",
    "        \n",
    "        # Process and plot behavior\n",
    "        behavior = preprocess.process_behavior(session_data, paths)\n",
    "        print('behavior done')\n",
    "        \n",
    "        # Save masks so can run in Matlab to process other planes\n",
    "        # preprocess.cell_masks(paths, 0)\n",
    "        print('masks saved')\n",
    "        \n",
    "        # Grab activity\n",
    "        deconvolved = preprocess.process_activity(paths, 'spks', 3, 0)\n",
    "        print('activity processed')\n",
    "        \n",
    "        # Normalize activity\n",
    "        norm_deconvolved = preprocess.normalize_deconvolved(deconvolved, behavior, paths, 0)\n",
    "        print('activity normalized')\n",
    "        \n",
    "        # Gaussian filter activity\n",
    "        norm_moving_deconvolved_filtered = preprocess.difference_gaussian_filter(norm_deconvolved, 4, behavior, paths, 0)\n",
    "        print('activity filtered')\n",
    "        \n",
    "        # Make trial-averaged traces and baseline subtract\n",
    "        mean_cs_1_responses_df = preprocess.normalized_trial_averaged(norm_deconvolved, behavior, 'cs_1')\n",
    "        mean_cs_2_responses_df = preprocess.normalized_trial_averaged(norm_deconvolved, behavior, 'cs_2')\n",
    "        print('traces done')\n",
    "        \n",
    "        # Get significant cells\n",
    "        [cs_1_poscells, cs_1_negcells] = preprocess.sig_test(norm_deconvolved, behavior, 'cs_1')\n",
    "        [cs_2_poscells, cs_2_negcells] = preprocess.sig_test(norm_deconvolved, behavior, 'cs_2')\n",
    "        [both_poscells, both_sigcells] = preprocess.combine_sig(cs_1_poscells, cs_1_negcells, cs_2_poscells, cs_2_negcells)\n",
    "        print('sig cells done')\n",
    "        \n",
    "        # Get index of top cell differences\n",
    "        idx = preprocess.get_index(behavior, mean_cs_1_responses_df, mean_cs_2_responses_df, cs_1_poscells, cs_2_poscells, both_poscells, both_sigcells, paths, 1)\n",
    "        print('idx done')\n",
    "        \n",
    "        # Get prior for synchronous cue activity\n",
    "        prior = classify.prior(norm_moving_deconvolved_filtered, idx['cs_1'], idx['cs_2'], behavior, [])\n",
    "        print('prior done')\n",
    "        \n",
    "        # Logistic regression\n",
    "        #y_pred, feature_importance_all = classify.log_regression_no_splits(behavior, norm_deconvolved, norm_moving_deconvolved_filtered, both_poscells, prior)\n",
    "        y_pred_log, feature_importance_all = classify.log_regression_splits(behavior, norm_deconvolved, norm_moving_deconvolved_filtered, both_poscells, prior)\n",
    "\n",
    "        print('logistic regression done')\n",
    "        \n",
    "        # Process classified output\n",
    "        y_pred = classify.process_classified(y_pred_log, prior, paths, 1)\n",
    "        print('classified done')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activity = norm_deconvolved.to_numpy()\n",
    "activity = activity[idx['both'].index]\n",
    "\n",
    "# Commented out code for filtering significant reactivated cells.\n",
    "# sig_cells = preprocess.sig_reactivated_cells([], [], [], [], [], paths, 0)\n",
    "# activity = activity[sig_cells > 0, :]\n",
    "\n",
    "# Initialize lists to store mean activities and onsets for two different cues.\n",
    "mean_activity_cs_1 = []\n",
    "mean_activity_cs_2 = []\n",
    "mean_activity_cs_any = []\n",
    "onsets_cs_1 = []\n",
    "onsets_cs_2 = []\n",
    "\n",
    "# Loop over each onset and offset in behavior data.\n",
    "for i in range(0, len(behavior['onsets'])):\n",
    "    # Calculate the mean activity for the given onset to offset period.\n",
    "    temp_activity = np.mean(activity[:, int(behavior['onsets'][i]):int(behavior['offsets'][i]) + 1], axis=1)\n",
    "    # Check if the cue code matches cs_1_code, and store the mean activity and onset.\n",
    "    if behavior['cue_codes'][i] == behavior['cs_1_code']:\n",
    "        mean_activity_cs_1.append(temp_activity)\n",
    "        onsets_cs_1.append(behavior['onsets'][i])\n",
    "        mean_activity_cs_any.append(temp_activity)\n",
    "    # Check if the cue code matches cs_2_code, and store the mean activity and onset.\n",
    "    if behavior['cue_codes'][i] == behavior['cs_2_code']:\n",
    "        mean_activity_cs_2.append(temp_activity)\n",
    "        onsets_cs_2.append(behavior['onsets'][i])\n",
    "        mean_activity_cs_any.append(temp_activity)\n",
    "\n",
    "cos_sim = lambda a, b: np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
    "topn = 0.05\n",
    "\n",
    "# Commented out code for filtering significant reactivated cells.\n",
    "# sig_cells = preprocess.sig_reactivated_cells([], [], [], [], [], paths, 0)\n",
    "# activity = activity[sig_cells > 0, :]\n",
    "\n",
    "# Initialize lists to store mean activities and onsets for two different cues.\n",
    "mean_activity_cs_1 = []\n",
    "mean_activity_cs_2 = []\n",
    "mean_activity_cs_any = []\n",
    "onsets_cs_1 = []\n",
    "onsets_cs_2 = []\n",
    "\n",
    "# Loop over each onset and offset in behavior data.\n",
    "for i in range(0, len(behavior['onsets'])):\n",
    "    # Calculate the mean activity for the given onset to offset period.\n",
    "    temp_activity = np.mean(activity[:, int(behavior['onsets'][i]):int(behavior['offsets'][i]) + 1], axis=1)\n",
    "    # Check if the cue code matches cs_1_code, and store the mean activity and onset.\n",
    "    if behavior['cue_codes'][i] == behavior['cs_1_code']:\n",
    "        mean_activity_cs_1.append(temp_activity)\n",
    "        onsets_cs_1.append(behavior['onsets'][i])\n",
    "        mean_activity_cs_any.append(temp_activity)\n",
    "    # Check if the cue code matches cs_2_code, and store the mean activity and onset.\n",
    "    if behavior['cue_codes'][i] == behavior['cs_2_code']:\n",
    "        mean_activity_cs_2.append(temp_activity)\n",
    "        onsets_cs_2.append(behavior['onsets'][i])\n",
    "        mean_activity_cs_any.append(temp_activity)\n",
    "\n",
    "# Normalize the mean activities for each trial.\n",
    "mean_activity_cs_1 = mean_activity_cs_1 / np.mean(mean_activity_cs_any, axis=0)\n",
    "mean_activity_cs_2 = mean_activity_cs_2 / np.mean(mean_activity_cs_any, axis=0)\n",
    "\n",
    "mean_activity_cs_1 = np.stack(mean_activity_cs_1, axis=0)\n",
    "mean_activity_cs_2 = np.stack(mean_activity_cs_2, axis=0)\n",
    "\n",
    "# Calculate the mean of the first and last three mean activities for cs_1 and cs_2.\n",
    "n_centroids = 3\n",
    "mean_activity_cs_1_mean = [np.mean(mean_activity_cs_1[0:n_centroids], axis=0), np.mean(mean_activity_cs_1[len(mean_activity_cs_1) - n_centroids:len(mean_activity_cs_1)], axis=0)]\n",
    "mean_activity_cs_2_mean = [np.mean(mean_activity_cs_2[0:n_centroids], axis=0), np.mean(mean_activity_cs_2[len(mean_activity_cs_2) - n_centroids:len(mean_activity_cs_2)], axis=0)]\n",
    "\n",
    "# Calculate the difference vectors between the early and late mean activities for cs_1 and cs_2.\n",
    "mean_activity_cs_1_vec = mean_activity_cs_1_mean[1] - mean_activity_cs_1_mean[0]\n",
    "mean_activity_cs_2_vec = mean_activity_cs_2_mean[1] - mean_activity_cs_2_mean[0]\n",
    "\n",
    "late_trails_1 = mean_activity_cs_1_mean[1]\n",
    "top_n_percent_indices_1 = np.argsort(late_trails_1)[-int(len(late_trails_1) * topn):]\n",
    "mean_activity_cstopn_1 = mean_activity_cs_1[:, top_n_percent_indices_1]\n",
    "mean_activity_cstopn_1_late = mean_activity_cs_1_mean[1][top_n_percent_indices_1]\n",
    "\n",
    "late_trails_2 = mean_activity_cs_2_mean[1]\n",
    "top_n_percent_indices_2 = np.argsort(late_trails_2)[-int(len(late_trails_2) * topn):]\n",
    "mean_activity_cstopn_2 = mean_activity_cs_2[:, top_n_percent_indices_2]\n",
    "mean_activity_cstopn_2_late = mean_activity_cs_2_mean[1][top_n_percent_indices_2]\n",
    "\n",
    "sim_cs_1 = np.zeros(len(mean_activity_cs_1))\n",
    "sim_cs_2 = np.zeros(len(mean_activity_cs_2))\n",
    "\n",
    "# Normalize the mean activities for each trial using the difference vectors.\n",
    "for i in range(0, len(mean_activity_cs_1)):\n",
    "    sim_cs_1[i] = cos_sim(mean_activity_cstopn_1[i], mean_activity_cstopn_1_late)\n",
    "    # sim_cs_1[i] = cos_sim(mean_activity_cs_1[i], mean_activity_cs_1_mean[1])\n",
    "    # sim_cs_1[i] = np.dot(mean_activity_cs_1[i], mean_activity_cs_1_vec) / np.linalg.norm(mean_activity_cs_1_vec)\n",
    "for i in range(0, len(mean_activity_cs_2)):\n",
    "    sim_cs_2[i] = cos_sim(mean_activity_cstopn_2[i], mean_activity_cstopn_2_late)\n",
    "    # sim_cs_2[i] = cos_sim(mean_activity_cs_2[i], mean_activity_cs_2_mean[1])\n",
    "    # sim_cs_2[i] = np.dot(mean_activity_cs_2[i], mean_activity_cs_2_vec) / np.linalg.norm(mean_activity_cs_2_vec)\n",
    "\n",
    "\n",
    "# Initialize lists to store mean activities and trial indices for reactivation events.\n",
    "mean_activity_r_1 = []\n",
    "mean_activity_r_2 = []\n",
    "mean_activity_r_any = []\n",
    "trial_r_1 = []\n",
    "trial_r_2 = []\n",
    "\n",
    "# Copy reactivation predictions for cs_1 and cs_2.\n",
    "reactivation_cs_1 = y_pred[:, 0].copy()\n",
    "reactivation_cs_2 = y_pred[:, 1].copy()\n",
    "\n",
    "# Set a threshold for reactivation probability.\n",
    "p_threshold = .75\n",
    "\n",
    "# Initialize variables for reactivation event detection.\n",
    "cs_1_peak = 0\n",
    "cs_2_peak = 0\n",
    "i = 0\n",
    "next_r = 0\n",
    "\n",
    "# Loop through reactivation predictions to detect reactivation events.\n",
    "while i < len(reactivation_cs_1) - 1:\n",
    "    # Check if there is a reactivation event.\n",
    "    if reactivation_cs_1[i] > 0 or reactivation_cs_2[i] > 0:\n",
    "        if next_r == 0:\n",
    "            r_start = i\n",
    "            next_r = 1\n",
    "        # Update peak reactivation values.\n",
    "        if reactivation_cs_1[i] > cs_1_peak:\n",
    "            cs_1_peak = reactivation_cs_1[i]\n",
    "        if reactivation_cs_2[i] > cs_2_peak:\n",
    "            cs_2_peak = reactivation_cs_2[i]\n",
    "        #print(reactivation_cs_1[i + 1], reactivation_cs_2[i + 1])\n",
    "        # Check if the reactivation event has ended.\n",
    "        if reactivation_cs_1[i + 1] <= 0.001 and reactivation_cs_2[i + 1]<= 0.001:\n",
    "            r_end = i + 1\n",
    "            next_r = 0\n",
    "            # Store mean activity and trial index for cs_1 reactivation events above the threshold.\n",
    "            if cs_1_peak > p_threshold:\n",
    "                for j in range(0, len(onsets_cs_1)):\n",
    "                    if r_start < onsets_cs_1[j] and r_start > onsets_cs_1[j-1] and r_start < onsets_cs_1[j-1] + int(behavior['framerate']*61):\n",
    "                        mean_activity_r_1.append(np.mean(activity[:, r_start:r_end], axis=1))\n",
    "                        trial_r_1.append(j-1)\n",
    "                        mean_activity_r_any.append(np.mean(activity[:, r_start:r_end], axis=1))\n",
    "                        break\n",
    "            # Store mean activity and trial index for cs_2 reactivation events above the threshold.\n",
    "            if cs_2_peak > p_threshold:\n",
    "                for j in range(0, len(onsets_cs_2)):\n",
    "                    if r_start < onsets_cs_2[j] and r_start > onsets_cs_2[j-1] and r_start < onsets_cs_2[j-1] + int(behavior['framerate']*61):\n",
    "                        mean_activity_r_2.append(np.mean(activity[:, r_start:r_end], axis=1))\n",
    "                        trial_r_2.append(j-1)\n",
    "                        mean_activity_r_any.append(np.mean(activity[:, r_start:r_end], axis=1))\n",
    "                        break\n",
    "            # Reset variables for the next reactivation event.\n",
    "            i = r_end\n",
    "            cs_1_peak = 0\n",
    "            cs_2_peak = 0\n",
    "    i += 1\n",
    "\n",
    "\n",
    "# Normalize the reactivation mean activities.\n",
    "mean_activity_r_1 = mean_activity_r_1 / np.mean(mean_activity_r_any, axis=0)\n",
    "mean_activity_r_2 = mean_activity_r_2 / np.mean(mean_activity_r_any, axis=0)\n",
    "\n",
    "mean_activity_r_1 = np.stack(mean_activity_r_1, axis=0)\n",
    "mean_activity_r_2 = np.stack(mean_activity_r_2, axis=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SERs drift over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.linalg import norm\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from scipy.stats import linregress\n",
    "from scipy.stats import t as t_dist\n",
    "\n",
    "\n",
    "def compute_drift_cosine(mean_activity, confidence=0.95, measure=\"distance\"):\n",
    "    \"\"\"\n",
    "    Computes a drift metric based on consecutive-trial cosine similarity.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    mean_activity : ndarray of shape (num_trials, num_neurons)\n",
    "        mean_activity[t] is the activity vector for trial t across all neurons.\n",
    "    \n",
    "    confidence : float, optional\n",
    "        Confidence level for confidence intervals. Default is 0.95 (95% CI).\n",
    "    \n",
    "    measure : {'distance', 'similarity'}, optional\n",
    "        Whether to return cosine distance (drift) or cosine similarity.\n",
    "        - 'distance' -> 1 - cos_sim\n",
    "        - 'similarity' -> cos_sim\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    drift_values : ndarray\n",
    "        An array of length (num_trials - 1) containing the drift or similarity\n",
    "        between consecutive trials.\n",
    "    \"\"\"\n",
    "\n",
    "    # Number of trials\n",
    "    n_trials = mean_activity.shape[0]\n",
    "    \n",
    "    # Compute cosine similarity for each pair of consecutive trials\n",
    "    cos_sims = []\n",
    "    for t in range(n_trials - 1):\n",
    "        vec_t = mean_activity[t]\n",
    "        vec_tp1 = mean_activity[t+1]\n",
    "        \n",
    "        # Safeguard against zero-norm vectors\n",
    "        denom = (norm(vec_t) * norm(vec_tp1))\n",
    "        if denom == 0:\n",
    "            cos_sims.append(np.nan)\n",
    "        else:\n",
    "            cos_sim = np.dot(vec_t, vec_tp1) / denom\n",
    "            cos_sims.append(cos_sim)\n",
    "    \n",
    "    cos_sims = np.array(cos_sims)\n",
    "    \n",
    "    if measure == \"distance\":\n",
    "        # Cosine distance = 1 - cosine similarity\n",
    "        drift_values = 1 - cos_sims\n",
    "        y_label = \"Cosine Distance (1 - cos_sim)\"\n",
    "        title_str = \"Cosine Distance (Drift) Across Trials\"\n",
    "    else:\n",
    "        # Directly use cosine similarity\n",
    "        drift_values = cos_sims\n",
    "        y_label = \"Cosine Similarity\"\n",
    "        title_str = \"Cosine Similarity Across Trials\"\n",
    "\n",
    "    # Print the drift/similarity values\n",
    "    #print(f\"{measure.capitalize()} values between consecutive trials:\")\n",
    "    #print(drift_values)\n",
    "\n",
    "    # Prepare data for linear regression\n",
    "    x = np.arange(len(drift_values)).reshape(-1, 1)\n",
    "    y = drift_values\n",
    "\n",
    "    # Fit a linear regression model\n",
    "    reg_model = LinearRegression()\n",
    "    reg_model.fit(x, y)\n",
    "    y_trend = reg_model.predict(x)\n",
    "\n",
    "    # Get slope, intercept, etc. via linregress\n",
    "    slope, intercept, r_value, p_value, std_err = linregress(x.flatten(), y)\n",
    "    print(f\"Slope: {slope}\")\n",
    "    print(f\"P-value: {p_value}\")\n",
    "\n",
    "    if p_value < 0.05:\n",
    "        print(\"The regression line significantly changes over time (p < 0.05).\")\n",
    "    else:\n",
    "        print(\"The regression line does not significantly change over time (p >= 0.05).\")\n",
    "\n",
    "    # Calculate confidence intervals for the trend line\n",
    "    n = len(y)\n",
    "    dof = n - 2  # degrees of freedom\n",
    "    t_crit = t_dist.ppf((1 + confidence) / 2, dof)  # critical t-value for given confidence level\n",
    "\n",
    "    # Standard error of predicted values\n",
    "    residuals = y - y_trend\n",
    "    s_err = np.sqrt(np.sum(residuals**2) / dof)\n",
    "\n",
    "    # Compute confidence interval for each predicted point\n",
    "    x_mean = np.mean(x)\n",
    "    x_var = np.sum((x.flatten() - x_mean)**2)\n",
    "    ci = t_crit * s_err * np.sqrt(1/n + ((x.flatten() - x_mean)**2 / x_var))\n",
    "\n",
    "    upper_bound = y_trend.flatten() + ci\n",
    "    lower_bound = y_trend.flatten() - ci\n",
    "\n",
    "    # Plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(drift_values, marker='o', linestyle='-', color='blue', label='Data')\n",
    "    plt.plot(x.flatten(), y_trend, color='red', linestyle='--', linewidth=2, label='Trend Line')\n",
    "    plt.fill_between(x.flatten(), lower_bound, upper_bound, color='red', alpha=0.2, \n",
    "                     label=f\"{int(confidence * 100)}% CI\")\n",
    "    plt.xlabel('Consecutive trial index (t)')\n",
    "    plt.ylabel(y_label)\n",
    "    plt.title(title_str)\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    #return drift_values\n",
    "\n",
    "\n",
    "compute_drift_cosine(mean_activity_cs_1)\n",
    "compute_drift_cosine(mean_activity_cs_2)\n",
    "compute_drift_cosine(mean_activity_r_1)\n",
    "compute_drift_cosine(mean_activity_r_2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reactivations most closely resemble the future SER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.stats import ttest_rel\n",
    "\n",
    "def plot_reactivation_similarities(\n",
    "    mean_activity_r, \n",
    "    trial_r, \n",
    "    mean_activity_cs, \n",
    "    name_r=\"Reactivations\", \n",
    "    name_cs=\"Stimulus\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Compute and plot the cosine similarity of each reactivation vector to\n",
    "    the previous, current, and next trial's mean activity vector, then\n",
    "    run paired t-tests comparing these similarity groups.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    mean_activity_r : np.ndarray\n",
    "        Shape (num_reactivations, num_neurons). Each row corresponds to a\n",
    "        reactivation event's mean activity across neurons.\n",
    "    \n",
    "    trial_r : array-like of int\n",
    "        Indices mapping each reactivation (row in mean_activity_r) to a\n",
    "        trial index. Must have the same length as mean_activity_r.\n",
    "    \n",
    "    mean_activity_cs : np.ndarray\n",
    "        Shape (num_trials, num_neurons). Each row corresponds to a trial's\n",
    "        mean activity across neurons (i.e., the 'stimulus' or 'trial' data).\n",
    "    \n",
    "    name_r : str, optional\n",
    "        Label for the reactivation data (used in plot titles and print-outs).\n",
    "        Default is \"Reactivations\".\n",
    "    \n",
    "    name_cs : str, optional\n",
    "        Label for the trial/stimulus data. Default is \"Stimulus\".\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    similarities_df : pd.DataFrame\n",
    "        A DataFrame containing:\n",
    "        \n",
    "        - \"Reactivation Index\": index of the reactivation event\n",
    "        - \"Trial Index\": the associated trial index\n",
    "        - \"Similarity to Previous Trial\": cosine similarity to trial-1 (NaN if none)\n",
    "        - \"Similarity to Current Trial\": cosine similarity to trial\n",
    "        - \"Similarity to Next Trial\": cosine similarity to trial+1 (NaN if none)\n",
    "    \"\"\"\n",
    "\n",
    "    # A list to store similarity results for each reactivation\n",
    "    reactivation_to_trial_similarities = []\n",
    "\n",
    "    # Iterate through each reactivation\n",
    "    for r_idx, reactivation_vector in enumerate(mean_activity_r):\n",
    "        # The trial index associated with this reactivation\n",
    "        trial_idx = trial_r[r_idx]\n",
    "\n",
    "        # Get the mean activity of the previous trial if it exists\n",
    "        if trial_idx > 0:\n",
    "            previous_trial_mean = mean_activity_cs[trial_idx - 1]\n",
    "        else:\n",
    "            previous_trial_mean = None\n",
    "\n",
    "        # Mean activity of the current trial\n",
    "        current_trial_mean = mean_activity_cs[trial_idx]\n",
    "\n",
    "        # Mean activity of the next trial if it exists\n",
    "        if trial_idx < len(mean_activity_cs) - 1:\n",
    "            next_trial_mean = mean_activity_cs[trial_idx + 1]\n",
    "        else:\n",
    "            next_trial_mean = None\n",
    "\n",
    "        # Compute cosine similarities, handling cases where prev/next trial might not exist\n",
    "        sim_prev = (cosine_similarity(\n",
    "                        reactivation_vector.reshape(1, -1), \n",
    "                        previous_trial_mean.reshape(1, -1)\n",
    "                    )[0, 0] if previous_trial_mean is not None else np.nan)\n",
    "        \n",
    "        sim_curr = cosine_similarity(\n",
    "                       reactivation_vector.reshape(1, -1), \n",
    "                       current_trial_mean.reshape(1, -1)\n",
    "                   )[0, 0]\n",
    "\n",
    "        sim_next = (cosine_similarity(\n",
    "                        reactivation_vector.reshape(1, -1), \n",
    "                        next_trial_mean.reshape(1, -1)\n",
    "                    )[0, 0] if next_trial_mean is not None else np.nan)\n",
    "\n",
    "        # Store the results in a dictionary\n",
    "        similarities = {\n",
    "            \"Reactivation Index\": r_idx,\n",
    "            \"Trial Index\": trial_idx,\n",
    "            \"Similarity to Previous Trial\": sim_prev,\n",
    "            \"Similarity to Current Trial\": sim_curr,\n",
    "            \"Similarity to Next Trial\": sim_next\n",
    "        }\n",
    "        reactivation_to_trial_similarities.append(similarities)\n",
    "\n",
    "    # Convert the list of dictionaries to a DataFrame\n",
    "    similarities_df = pd.DataFrame(reactivation_to_trial_similarities)\n",
    "\n",
    "    # --- OPTIONAL ---\n",
    "    # If you need to drop the first row (for example, if reactivation index 0 is invalid):\n",
    "    # similarities_df = similarities_df.drop(index=0).reset_index(drop=True)\n",
    "\n",
    "    # ---- PAIRED T-TESTS ----\n",
    "    # Drop rows with NaNs in pairs so that the same indices are used for each pairwise comparison.\n",
    "    curr_prev_df = similarities_df.dropna(\n",
    "        subset=[\"Similarity to Current Trial\", \"Similarity to Previous Trial\"]\n",
    "    )\n",
    "    curr_next_df = similarities_df.dropna(\n",
    "        subset=[\"Similarity to Current Trial\", \"Similarity to Next Trial\"]\n",
    "    )\n",
    "    prev_next_df = similarities_df.dropna(\n",
    "        subset=[\"Similarity to Previous Trial\", \"Similarity to Next Trial\"]\n",
    "    )\n",
    "\n",
    "    # Perform the paired t-tests\n",
    "    current_vs_previous = ttest_rel(\n",
    "        curr_prev_df[\"Similarity to Current Trial\"],\n",
    "        curr_prev_df[\"Similarity to Previous Trial\"]\n",
    "    )\n",
    "    current_vs_next = ttest_rel(\n",
    "        curr_next_df[\"Similarity to Current Trial\"],\n",
    "        curr_next_df[\"Similarity to Next Trial\"]\n",
    "    )\n",
    "    previous_vs_next = ttest_rel(\n",
    "        prev_next_df[\"Similarity to Previous Trial\"],\n",
    "        prev_next_df[\"Similarity to Next Trial\"]\n",
    "    )\n",
    "\n",
    "    # Print T-test results\n",
    "    print(f\"T-test ({name_r} & {name_cs}): Current vs Previous Trial Similarities: {current_vs_previous}\")\n",
    "    print(f\"T-test ({name_r} & {name_cs}): Current vs Next Trial Similarities: {current_vs_next}\")\n",
    "    print(f\"T-test ({name_r} & {name_cs}): Previous vs Next Trial Similarities: {previous_vs_next}\")\n",
    "\n",
    "    # Display the final DataFrame\n",
    "    print(\"\\nSimilarity DataFrame:\\n\", similarities_df)\n",
    "\n",
    "    # ---- PLOTTING ----\n",
    "    # 1) Line plot across reactivations\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(\n",
    "        similarities_df[\"Reactivation Index\"],\n",
    "        similarities_df[\"Similarity to Previous Trial\"],\n",
    "        label=\"Similarity to Previous Trial\", color=\"blue\", alpha=0.7\n",
    "    )\n",
    "    plt.plot(\n",
    "        similarities_df[\"Reactivation Index\"],\n",
    "        similarities_df[\"Similarity to Current Trial\"],\n",
    "        label=\"Similarity to Current Trial\", color=\"green\", alpha=0.7\n",
    "    )\n",
    "    plt.plot(\n",
    "        similarities_df[\"Reactivation Index\"],\n",
    "        similarities_df[\"Similarity to Next Trial\"],\n",
    "        label=\"Similarity to Next Trial\", color=\"orange\", alpha=0.7\n",
    "    )\n",
    "    plt.xlabel(\"Reactivation Index\")\n",
    "    plt.ylabel(\"Cosine Similarity\")\n",
    "    plt.title(f\"{name_r} Similarities to Neighboring {name_cs} Trials\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    # 2) Box plot of the three similarity distributions\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    # Melt the DataFrame to long form for seaborn\n",
    "    melted_df = similarities_df.melt(\n",
    "        id_vars=[\"Reactivation Index\", \"Trial Index\"],\n",
    "        value_vars=[\n",
    "            \"Similarity to Previous Trial\",\n",
    "            \"Similarity to Current Trial\",\n",
    "            \"Similarity to Next Trial\",\n",
    "        ],\n",
    "        var_name=\"Trial Type\",\n",
    "        value_name=\"Cosine Similarity\",\n",
    "    )\n",
    "    sns.boxplot(data=melted_df, x=\"Trial Type\", y=\"Cosine Similarity\")\n",
    "    plt.title(f\"Distribution of {name_r} Cosine Similarities\\nto Neighboring {name_cs} Trials\")\n",
    "    plt.xlabel(\"Trial Type\")\n",
    "    plt.ylabel(\"Cosine Similarity\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    # Return the DataFrame if further analysis or saving is required\n",
    "    return similarities_df\n",
    "\n",
    "\n",
    "plot_reactivation_similarities(mean_activity_r_1, trial_r_1, mean_activity_cs_1, \"Reactivations Stimulus 1\", \"SER Stimulus 1\")\n",
    "plot_reactivation_similarities(mean_activity_r_2, trial_r_2, mean_activity_cs_2, \"Reactivations Stimulus 2\", \"SER Stimulus 2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting sign of change of SER per neuron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "\n",
    "\n",
    "def evaluate_fold(model, X_train, X_test, y_train, y_test, test_idx):\n",
    "    \"\"\"Trains the model and evaluates metrics for a single CV fold.\n",
    "       Also returns predictions and test indices so we can track\n",
    "       which events (rows) were in this fold.\n",
    "    \"\"\"\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Compute metrics for each output (neuron)\n",
    "    fold_accuracies = []\n",
    "    fold_precisions = []\n",
    "    fold_recalls = []\n",
    "    fold_f1s = []\n",
    "\n",
    "    for neuron_idx in range(y_test.shape[1]):\n",
    "        y_test_n = y_test[:, neuron_idx]\n",
    "        y_pred_n = y_pred[:, neuron_idx]\n",
    "\n",
    "        fold_accuracies.append(accuracy_score(y_test_n, y_pred_n))\n",
    "        fold_precisions.append(precision_score(y_test_n, y_pred_n, zero_division=0))\n",
    "        fold_recalls.append(recall_score(y_test_n, y_pred_n, zero_division=0))\n",
    "        fold_f1s.append(f1_score(y_test_n, y_pred_n, zero_division=0))\n",
    "\n",
    "    return (fold_accuracies, fold_precisions, fold_recalls, fold_f1s,\n",
    "            y_test, y_pred, test_idx)\n",
    "\n",
    "\n",
    "def LR_model(X_all, y_all):\n",
    "    print(f\"X_all shape: {X_all.shape}, y_all shape: {y_all.shape}\")\n",
    "\n",
    "    # Initialize KFold\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "    # Model initialization\n",
    "    base_model = LogisticRegression(\n",
    "        max_iter=1000, \n",
    "        random_state=42,\n",
    "        solver=\"lbfgs\",\n",
    "        penalty=\"l2\", \n",
    "        class_weight=\"balanced\"\n",
    "    )\n",
    "    model_cv = MultiOutputClassifier(base_model)\n",
    "\n",
    "    # Parallel cross-validation\n",
    "    all_results = Parallel(n_jobs=-1)(\n",
    "        delayed(evaluate_fold)(\n",
    "            model_cv,\n",
    "            X_all[train_idx], X_all[test_idx],\n",
    "            y_all[train_idx], y_all[test_idx],\n",
    "            test_idx\n",
    "        )\n",
    "        for train_idx, test_idx in kf.split(X_all)\n",
    "    )\n",
    "\n",
    "    # Unpack results\n",
    "    (all_accuracies, all_precisions, all_recalls, all_f1s,\n",
    "     all_y_test, all_y_pred, all_test_idx) = zip(*all_results)\n",
    "\n",
    "    # Flatten or concatenate the metrics across folds\n",
    "    all_accuracies = np.concatenate(all_accuracies)\n",
    "    all_precisions = np.concatenate(all_precisions)\n",
    "    all_recalls = np.concatenate(all_recalls)\n",
    "    all_f1s = np.concatenate(all_f1s)\n",
    "\n",
    "    for fold_idx, test_indices in enumerate(all_test_idx):\n",
    "        y_pred_fold = all_y_pred[fold_idx]  # shape: (len(test_indices), n_outputs)\n",
    "        y_test_fold = all_y_test[fold_idx]  # shape: (len(test_indices), n_outputs)\n",
    "\n",
    "    # --- Summaries ---\n",
    "    print(\"\\n=== Cross-Validation Summary ===\")\n",
    "    print(f\"Accuracy:  {np.mean(all_accuracies):.3f} ± {np.std(all_accuracies):.3f}\")\n",
    "    print(f\"Precision: {np.mean(all_precisions):.3f}\")\n",
    "    print(f\"Recall:    {np.mean(all_recalls):.3f}\")\n",
    "    print(f\"F1-score:  {np.mean(all_f1s):.3f}\")\n",
    "\n",
    "    metrics_dict = {\n",
    "        \"Accuracy\": all_accuracies,\n",
    "        \"Precision\": all_precisions,\n",
    "        \"Recall\": all_recalls,\n",
    "        \"F1\": all_f1s\n",
    "    }\n",
    "\n",
    "    # Plot distributions\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    for i, (metric_name, values) in enumerate(metrics_dict.items()):\n",
    "        plt.subplot(2, 2, i+1)\n",
    "        plt.hist(values, bins=100, alpha=0.7, color='b')\n",
    "        plt.title(f\"Distribution of {metric_name} (all folds, all neurons)\")\n",
    "        plt.xlabel(metric_name)\n",
    "        plt.ylabel(\"Count\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_1 = np.diff(mean_activity_cs_1, axis= 0)\n",
    "x_2 = np.diff(mean_activity_cs_2, axis=0)\n",
    "\n",
    "X_all = np.concatenate([x_1[:-1, ], x_2[:-1, ]])\n",
    "y_1 = np.diff(mean_activity_cs_1, axis=0)\n",
    "y_2 = np.diff(mean_activity_cs_2, axis=0)\n",
    "#taget SERt+1\n",
    "y_all = np.concatenate([y_1[1:, ], y_2[1:, ]])\n",
    "\n",
    "sign_changes = np.where(y_all > 0, 1, -1)\n",
    "print(sign_changes)\n",
    "\n",
    "#SERt - SERt-1\n",
    "LR_model(X_all, sign_changes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_sign_changes(mean_activity_cs, trial_r):\n",
    "    \"\"\"\n",
    "    Compute sign changes based on trial-to-trial differences and map them to reactivations.\n",
    "    \n",
    "    Parameters:\n",
    "    - mean_activity_cs: Trial-level activity (shape: trials x neurons)\n",
    "    - trial_r: Reactivation-to-trial mapping (shape: reactivations,)\n",
    "    \n",
    "    Returns:\n",
    "    - sign_changes: Array of sign changes mapped to reactivations (-1 or +1) for each neuron wrt the next SER trial.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Compute trial-to-trial differences\n",
    "    trial_diff = np.diff(mean_activity_cs, axis=0)  # Shape: (trials-1, neurons)\n",
    "\n",
    "    # Compute signs of trial differences\n",
    "    trial_sign_changes = np.where(trial_diff > 0, 1, -1)  # Shape: (trials-1, neurons)\n",
    "\n",
    "    # Map trial-level changes to reactivations\n",
    "    reactivation_sign_changes = trial_sign_changes[trial_r] \n",
    "\n",
    "    return reactivation_sign_changes\n",
    "\n",
    "def clean_and_prepare_data(mean_activity_r, mean_activity_cs, trial_r):\n",
    "    \"\"\"\n",
    "    Prepare predictors (X) and targets (y) for logistic regression\n",
    "    \n",
    "    Parameters:\n",
    "    - mean_activity_r: Reactivation-level activity (shape: reactivations x neurons)\n",
    "    - mean_activity_cs: Trial-level activity (shape: trials x neurons)\n",
    "    - trial_r: Reactivation-to-trial mapping (shape: reactivations,)\n",
    "    \n",
    "    Returns:\n",
    "    - X_all: Combined predictors (reactivation activity) for sensory evoked response (shape: total_reactivations, neurons).\n",
    "    - y_all: Combined targets (sign changes) for all neurons (shape: total_reactivations, neurons).\n",
    "    \"\"\"\n",
    "    X_list = []  # Predictors for all neurons\n",
    "    y_list = []  # Targets for all neurons\n",
    "    \n",
    "    # Compute targets for all neurons\n",
    "    for neuron in range(mean_activity_cs_1.shape[1]):\n",
    "        y_neuron = compute_sign_changes(mean_activity_cs[:, neuron], trial_r)\n",
    "        y_list.append(y_neuron)\n",
    "\n",
    "    # Prepare predictors for the subset of neurons\n",
    "    for neuron in range(mean_activity_r.shape[1]):\n",
    "        # Adjust reactivation activity by subtracting the SER activity\n",
    "        X_neuron = mean_activity_r[:, neuron] - mean_activity_cs[trial_r, neuron]\n",
    "        X_list.append(X_neuron)\n",
    "    \n",
    "    # Stack all predictors and targets\n",
    "    X_all = np.column_stack(X_list)  # Shape: (total_reactivations, subset_neurons)\n",
    "    y_all = np.column_stack(y_list)  # Shape: (total_reactivations, total_neurons)\n",
    "        \n",
    "    return X_all, y_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_cs_1, y_cs_1_clean = clean_and_prepare_data(mean_activity_r_1, mean_activity_cs_1, trial_r_1)\n",
    "X_cs_2, y_cs_2_clean = clean_and_prepare_data(mean_activity_r_2, mean_activity_cs_2, trial_r_2)\n",
    "\n",
    "X_all = np.concatenate([mean_activity_r_1, mean_activity_r_2])  # Shape: (total_reactivations, neurons)\n",
    "y_all = np.concatenate([y_cs_1_clean, y_cs_2_clean]) \n",
    "\n",
    "#raw reactivation data (projected in reactivation space)\n",
    "LR_model(X_all, y_all)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ri - SERt-1 (projected in reactivation space)\n",
    "X_cs_1, y_cs_1_clean = clean_and_prepare_data(mean_activity_r_1, mean_activity_cs_1, trial_r_1)\n",
    "X_cs_2, y_cs_2_clean = clean_and_prepare_data(mean_activity_r_2, mean_activity_cs_2, trial_r_2)\n",
    "\n",
    "# Combine the datasets\n",
    "X_all = np.concatenate([X_cs_1, X_cs_2])  # Shape: (total_reactivations, neurons)\n",
    "y_all = np.concatenate([y_cs_1_clean, y_cs_2_clean]) \n",
    "\n",
    "print(X_all.shape, y_all.shape)\n",
    "\n",
    "# Sanity check the processed targets\n",
    "unique_cs_1, counts_cs_1 = np.unique(y_cs_1_clean, return_counts=True)\n",
    "unique_cs_2, counts_cs_2 = np.unique(y_cs_2_clean, return_counts=True)\n",
    "\n",
    "print(\"Processed Targets for CS_1:\")\n",
    "print(dict(zip(unique_cs_1, counts_cs_1)))\n",
    "print(\"\\nProcessed Targets for CS_2:\")\n",
    "print(dict(zip(unique_cs_2, counts_cs_2)))\n",
    "\n",
    "#Ri - SERt-1 (projected in reactivation space)\n",
    "LR_model(X_all, y_all)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ri - SERt-1 (compressed in SER space)\n",
    "\n",
    "def mean_reactivations_by_trial(mean_activity_r, trial_r, n_trials):\n",
    "   \n",
    "    n_reactivations, n_neurons = mean_activity_r.shape\n",
    "    \n",
    "    # Accumulate sums of reactivation activity per trial\n",
    "    trial_sums = np.zeros((n_trials, n_neurons), dtype=float)\n",
    "    reactivation_counts = np.zeros(n_trials, dtype=int)\n",
    "    \n",
    "    for i in range(n_reactivations):\n",
    "        trial_index = trial_r[i]\n",
    "        trial_sums[trial_index] += mean_activity_r[i]\n",
    "        reactivation_counts[trial_index] += 1\n",
    "\n",
    "    # Compute mean, handling zero-reactivation trials\n",
    "    trial_means = np.zeros_like(trial_sums)\n",
    "    for t in range(n_trials):\n",
    "        if reactivation_counts[t] > 0:\n",
    "            trial_means[t] = trial_sums[t] / reactivation_counts[t]\n",
    "        else:\n",
    "            trial_means[t] = 0\n",
    "    \n",
    "    return trial_means\n",
    "\n",
    "mean_mean_activity_r1 = mean_reactivations_by_trial(mean_activity_r_1, trial_r_1, n_trials=mean_activity_cs_1.shape[0])\n",
    "print(mean_mean_activity_r1.shape)\n",
    "mean_mean_activity_r2 = mean_reactivations_by_trial(mean_activity_r_2, trial_r_2, n_trials=mean_activity_cs_2.shape[0])\n",
    "print(mean_mean_activity_r2.shape)\n",
    "\n",
    "X_1 = mean_mean_activity_r1 - mean_activity_cs_1\n",
    "X_2 = mean_mean_activity_r2 - mean_activity_cs_2\n",
    "\n",
    "x_all = np.concatenate([X_1[:-1, ], X_2[:-1, ]])\n",
    "print(x_all.shape)\n",
    "\n",
    "y_1 = np.diff(mean_activity_cs_1, axis=0)\n",
    "y_2 = np.diff(mean_activity_cs_2, axis=0)\n",
    "\n",
    "y_all = np.concatenate([y_1, y_2])\n",
    "\n",
    "sign_changes = np.where(y_all > 0, 1, -1)\n",
    "print(sign_changes)\n",
    "\n",
    "#Ri - SERt-1 (compressed in SER space)\n",
    "LR_model(x_all, sign_changes)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_cs_1, y_cs_1_clean = clean_and_prepare_data(mean_activity_r_1, mean_activity_cs_1, trial_r_1)\n",
    "X_cs_2, y_cs_2_clean = clean_and_prepare_data(mean_activity_r_2, mean_activity_cs_2, trial_r_2)\n",
    "\n",
    "X_cs_1_diff = np.diff(mean_activity_r_1, axis=0)\n",
    "X_cs_2_diff = np.diff(mean_activity_r_2, axis=0)\n",
    "\n",
    "X_all = np.concatenate([X_cs_1_diff, X_cs_2_diff])  # Shape: (total_reactivations -2, neurons)\n",
    "y_all = np.concatenate([y_cs_1_clean[:-1, ], y_cs_2_clean[:-1, ]]) \n",
    "\n",
    "#Ri - Ri-1 (in reactivation space)\n",
    "LR_model(X_all, y_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting sign of change per neuron per reactivation event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_all = np.concatenate([mean_activity_r_1[:-1, ], mean_activity_r_2[:-1, ]])\n",
    "y_1 = np.diff(mean_activity_r_1, axis=0)\n",
    "y_2 = np.diff(mean_activity_r_2, axis=0)\n",
    "y_all = np.concatenate([y_1, y_2])\n",
    "\n",
    "sign_changes = np.where(y_all > 0, 1, -1)\n",
    "print(sign_changes)\n",
    "\n",
    "#raw Ri-1 data (in R space)\n",
    "LR_model(X_all, sign_changes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_1 = np.diff(mean_activity_r_1, axis= 0)\n",
    "x_2 = np.diff(mean_activity_r_2, axis=0)\n",
    "\n",
    "X_all = np.concatenate([x_1[:-1, ], x_2[:-1, ]])\n",
    "y_1 = np.diff(mean_activity_r_1, axis=0)\n",
    "y_2 = np.diff(mean_activity_r_2, axis=0)\n",
    "#taget Ri+1\n",
    "y_all = np.concatenate([y_1[1:, ], y_2[1:, ]])\n",
    "\n",
    "sign_changes = np.where(y_all > 0, 1, -1)\n",
    "print(sign_changes)\n",
    "\n",
    "#Ri - Ri-1\n",
    "LR_model(X_all, sign_changes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_sign_changes_r(mean_activity_r):\n",
    "    \"\"\"\n",
    "    Compute sign changes based on trial-to-trial differences and map them to reactivations.\n",
    "    \n",
    "    Parameters:\n",
    "    - mean_activity_cs: Trial-level activity (shape: trials x neurons)\n",
    "    - trial_r: Reactivation-to-trial mapping (shape: reactivations,)\n",
    "    \n",
    "    Returns:\n",
    "    - sign_changes: Array of sign changes mapped to reactivations (-1 or +1) for each neuron wrt the next SER trial.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Compute trial-to-trial differences\n",
    "    trial_diff = np.diff(mean_activity_r, axis=0)  # Shape: (trials-1, neurons)\n",
    "\n",
    "    # Compute signs of trial differences\n",
    "    trial_sign_changes = np.where(trial_diff > 0, 1, -1)  # Shape: (trials-1, neurons)\n",
    "\n",
    "    # Map trial-level changes to reactivations\n",
    "    #reactivation_sign_changes = trial_sign_changes[trial_r]\n",
    "\n",
    "    return trial_sign_changes #reactivation_sign_changes\n",
    "\n",
    "def clean_and_prepare_data_r(mean_activity_r, mean_activity_cs, trial_r):\n",
    "    \"\"\"\n",
    "    Prepare predictors (X) and targets (y) for logistic regression\n",
    "    \n",
    "    Parameters:\n",
    "    - mean_activity_r: Reactivation-level activity (shape: reactivations x neurons)\n",
    "    - mean_activity_cs: Trial-level activity (shape: trials x neurons)\n",
    "    - trial_r: Reactivation-to-trial mapping (shape: reactivations,)\n",
    "    \n",
    "    Returns:\n",
    "    - X_all: Combined predictors (reactivation activity) for sensory evoked response (shape: total_reactivations, neurons).\n",
    "    - y_all: Combined targets (sign changes) for all neurons (shape: total_reactivations, neurons).\n",
    "    \"\"\"\n",
    "    X_list = []  # Predictors for all neurons\n",
    "    y_list = []  # Targets for all neurons\n",
    "    \n",
    "    # Compute targets for all neurons\n",
    "    for neuron in range(mean_activity_r.shape[1]):\n",
    "        y_neuron = compute_sign_changes_r(mean_activity_r[:, neuron])\n",
    "        y_list.append(y_neuron)\n",
    "\n",
    "    # Prepare predictors for the subset of neurons\n",
    "    for neuron in range(mean_activity_r.shape[1]):\n",
    "        # Adjust SER activity by subtracting the reactivation activity\n",
    "        X_neuron = mean_activity_cs[trial_r, neuron] - mean_activity_r[:, neuron]\n",
    "        X_list.append(X_neuron)\n",
    "    \n",
    "    # Stack all predictors and targets\n",
    "    X_all = np.column_stack(X_list)  # Shape: (total_reactivations, subset_neurons)\n",
    "    y_all = np.column_stack(y_list)  # Shape: (total_reactivations, total_neurons)\n",
    "        \n",
    "    return X_all, y_all\n",
    "\n",
    "\n",
    "X_cs_1, y_cs_1_clean = clean_and_prepare_data_r(mean_activity_r_1, mean_activity_cs_1, trial_r_1)\n",
    "X_cs_2, y_cs_2_clean = clean_and_prepare_data_r(mean_activity_r_2, mean_activity_cs_2, trial_r_2)\n",
    "\n",
    "# Combine the datasets\n",
    "X_all = np.concatenate([X_cs_1[:-1, ], X_cs_2[:-1, ]])  # Shape: (total_reactivations, neurons)\n",
    "y_all = np.concatenate([y_cs_1_clean, y_cs_2_clean]) \n",
    "\n",
    "print(X_all.shape, y_all.shape)\n",
    "\n",
    "# Sanity check the processed targets\n",
    "unique_cs_1, counts_cs_1 = np.unique(y_cs_1_clean, return_counts=True)\n",
    "unique_cs_2, counts_cs_2 = np.unique(y_cs_2_clean, return_counts=True)\n",
    "\n",
    "print(\"Processed Targets for CS_1:\")\n",
    "print(dict(zip(unique_cs_1, counts_cs_1)))\n",
    "print(\"\\nProcessed Targets for CS_2:\")\n",
    "print(dict(zip(unique_cs_2, counts_cs_2)))\n",
    "\n",
    "#SER - Ri-1 (projected in reactivation space, prediction reactivations with SER)\n",
    "LR_model(X_all, y_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_cs_1, y_cs_1_clean = clean_and_prepare_data_r(mean_activity_r_1, mean_activity_cs_1, trial_r_1)\n",
    "X_cs_2, y_cs_2_clean = clean_and_prepare_data_r(mean_activity_r_2, mean_activity_cs_2, trial_r_2)\n",
    "\n",
    "X_all = np.concatenate([mean_activity_cs_1[trial_r_1[:-1], ], mean_activity_cs_2[trial_r_2[:-1], ]])  # Shape: (total_reactivations, neurons)\n",
    "y_all = np.concatenate([y_cs_1_clean, y_cs_2_clean]) \n",
    "\n",
    "#raw SER data (projected in reactivation space)\n",
    "LR_model(X_all, y_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_cs_1, y_cs_1_clean = clean_and_prepare_data_r(mean_activity_r_1, mean_activity_cs_1, trial_r_1)\n",
    "X_cs_2, y_cs_2_clean = clean_and_prepare_data_r(mean_activity_r_2, mean_activity_cs_2, trial_r_2)\n",
    "\n",
    "X_cs_1_diff = np.diff(mean_activity_cs_1[trial_r_1, ], axis=0)\n",
    "X_cs_2_diff = np.diff(mean_activity_cs_2[trial_r_2, ], axis=0)\n",
    "\n",
    "X_all = np.concatenate([X_cs_1_diff, X_cs_2_diff]) \n",
    "y_all = np.concatenate([y_cs_1_clean, y_cs_2_clean]) \n",
    "\n",
    "#SERt - SERt-1 (in reactivation space)\n",
    "LR_model(X_all, y_all)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
